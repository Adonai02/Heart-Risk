---
output: github_document
---



```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


```{r include = FALSE}
library(DBI)
library(ggplot2)
library(tidyverse)
connect_own<-function() {
  
  options(mysql = list(
    "host" = "localhost", 
    "port" = 3306,
    "user" = "adonai",
    "password" = "huevos123"
    
  ))
  
  
  db <- RMySQL::dbConnect(RMySQL::MySQL(), dbname = "Curso_R",
                          host = options()$mysql$host, 
                          port = options()$mysql$port, user = options()$mysql$user, 
                          password = options()$mysql$password)  
  
  
  
  return(db)
}

con_own <- connect_own()
table <- dbReadTable(con_own, "Tabla_Proyecto_5")
data <- data.frame(table)
```
# Heart Risk



The goal of Heart-Risk is to do an analysis to know which factors are associates with our variable of interest, in this case, we want to know which variables affect the risk of having a heart disease.
We'll elaborate a logistic regression model, and we'll see the accuracy of the model with some variables taken with a simple intuition and logic, doing this, we will be forced to understand the context and just don't be so dependent of the functions that we already know, and above all, do something (a little) different.


***

## Export and visualitation of data


First, We going to export the dataset from MYSQL using the package DBI, the head of the dataset 
is following:

```{r echo=FALSE}
head(data)
```

Let's describe some variables:

* **Class:** This variable classifies from 0 to 4 the diagnostic or probably of having the disease, where 0 means No sick, 1, 2 and 3 means little probably having disease and 4 means a lot of probably having disease.

* **Thalach:** Maximum blood pressure found.

* **Chol:** Cholesterol in mg/dl.

* **Fbs:** Blood pressure Fasting.

* **Trestbps:** Blood pressure resting.

* **Testecg:** Induced blood acceleration.

* **Oldpeak:** Induced depression for relative exercise to test.

* **Sex:** The sex of the people classify in Female and Male.

* **Age:** The age of the people.

Once We known the decription of the variables, now, as we want to know if a person might have 
Heart disease or not, so, We will tranform to binary the *class* variable, and as factor the
variable *sex*  for work better with the data.


```{r pressure, echo = FALSE}
data["class_binomial"] <- ifelse(data$class==0, 0, 1)
data <- data %>% mutate(sex = factor(sex, labels = c("Female", "Male"))) #Preguntar a Roberto
str(data)
```


As you can see We already have converted the class and sex variables, now, let's identify the **Relevant** variables.

***

## Exploring the data

* Recommendation: Review concepts of t-test (Welch's t-test) and Chi-squared to identify relevant variables. 

**a)** We test the Chi function between the variables *Sex* and *Class binomial*, we obtain.

```{r echo=FALSE}
chisq.test(data$sex, data$class_binomial)
```


**b)** We test the t-test function between the variables *Age* and *Class binomial*, we obtain.

```{r echo=FALSE}
t.test(data$age, data$class_binomial)
```


**c)** We test the t-test function between the variables *Thalach* and *Class binomial*, we obtain.

```{r echo=FALSE}
t.test(data$thalach, data$class_binomial)
```

We can see the p-values of the three tests, and we can observe that its values are very small (which this is very excellent).

***

## Visualitation of data

Let's plot some relevant variables to have an idea about the distribution of it.

```{r echo=FALSE}
data["hd"] <- ifelse(data$class_binomial==1, "Disease", "No Disease")
ggplot(data, aes(x=hd, y=age) ) + geom_boxplot() + ggtitle("Boxplot Class binomial vs Age") +xlab("Class binomial") + ylab("Age")
```


```{r echo=FALSE}

ggplot(data, aes(x=hd, y=sex, fill=sex) ) + geom_bar(stat="identity") + ggtitle("Barplot Sex vs Class binomial") + xlab("Class binomial") + ylab("Sex")

```


```{r echo=FALSE}

ggplot(data, aes(x=hd, y=thalach) ) + geom_boxplot() + ggtitle("Barplot Thalach vs Class binomial") + xlab("Class binomial") + ylab("Thalach")

```


We can see that the range between 50 and 60 age are important for having a disease, the same thing happen with the range between 125 and 150 maximum blood pressure, on the other hand the sex doesn't care too much, We can say that the sex variable it's independent.


***

## Logistic Regression 

```{r echo=FALSE}
model <- glm(formula = class_binomial ~ age + thalach + sex,
             data = data, family = "binomial")

summary(model)

```

We note that, in our model, the *SexMale* and *Thalach* variables stand out, their p-values are tinny but, the coefficient of *Thalach* is almost zero on another hand with sexMale, while *SexMale's*' coefficient is positive, obviously, the model can be much better.

***

## Useful information about the model

```{r echo=FALSE}
require(broom)

data_model <- model%>% tidy()

column_new <- exp(cbind(OR = coef(model), confint(model)))
columns_news <- as_tibble(column_new)
data_model %>% add_column(columns_news)


```


Once We have some significance values of the model, like, p-values, standard deviations, Odds ratios and its confidence intervals, We can lead to some conclusions.
We note that, for example, the age doesn't keep much association with the dependent variable, it's because the OR is almost equal to 1 and the confidence interval is 1.00 - 1.07, that indicates the must be in this interval, On another hand, We notice in *sexMale* the OR is greater than 1, which it means that the association between the dependent variable is high, and the range of confidence interval it's not too higher.


***

## Predictive probabilities of the model

```{r echo=FALSE}
pred_probs <- predict(model, data, type = "response")

pred_bool <- ifelse(pred_probs > 0.5, 1, 0)

library(Metrics)
accuracy(pred_bool, data$class_binomial)*100

```

We observe the accuracy of the model it's not bad, but, it's not the result that We expected, is obviously that the model can be better, and it's because We didn't do an in-depth analysis, We led for our intuitions about what variable will fit better in the model and the accuracy  it's not too bad taking this into account. Without doubt, We have to improve the model or find another that fit better for our data. The learning We take away from this is gold, and We'll serve to make better models or take better decisions about the variables.
